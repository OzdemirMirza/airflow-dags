# âš¡ Orchestrating Spark on Kubernetes with Airflow

This project demonstrates how to orchestrate **Apache Spark jobs** on a **Kubernetes cluster** using **Apache Airflow**.  
Airflow DAGs are synced from a Git repository (via `git-sync`) and executed on demand through the Airflow UI.  

---

## ğŸš€ Setup & Workflow

1. âš™ï¸ **Deploy Airflow** with `KubernetesExecutor`
2. ğŸ”„ **Enable git-sync** â†’ DAGs auto-pulled from this repo
3. â˜¸ï¸ **Install Spark Operator** via Helm to manage Spark applications
4. ğŸ³ Adjust `values.yaml` for:
   - Docker image configuration
   - KubernetesExecutor settings
   - Git-Sync sidecar
5. ğŸª„ Create a simple DAG (`spark_pi`) that submits a Spark job (`spark-pi.yaml`) to the `spark-operator` namespace
6. â–¶ï¸ Trigger the DAG from **Airflow UI** â†’ Spark job executed successfully ğŸ‰

---

## ğŸ“‚ Project Structure

```bash
airflow-dags/
â”œâ”€â”€ kubernetes/
â”‚   â””â”€â”€ spark-pi.yaml        # SparkApplication definition
â”œâ”€â”€ dag_infrastructure_validation.py
â””â”€â”€ README.md
```

---
## ğŸ“Š Demo

After configuring Airflow with KubernetesExecutor and Spark Operator,
we successfully submitted and monitored a Spark job from the Airflow UI.

<p align="center">
  <img src="images/spark-pi-graph.png" alt="DAG Graph" width="720">
</p>

<p align="center">
  <img src="images/task-success.png" alt="Task Success" width="420">
</p>

---

ğŸ› ï¸ Key Technologies
	â€¢	âœˆï¸ Apache Airflow (KubernetesExecutor, Git-Sync)
	â€¢	â˜¸ï¸ Kubernetes (Kind cluster for local testing)
	â€¢	ğŸ”¥ Spark Operator (running Spark jobs as CRDs)
	â€¢	ğŸ³ Docker (base images for Airflow and Spark)


---

ğŸ“ How to Run

    1. Deploy Airflow with `KubernetesExecutor` and git-sync enabled  
      helm install airflow apache-airflow/airflow -f values.yaml

	2.	Install Spark Operator   
      helm install spark-operator spark-operator/spark-operator --namespace spark-operator

    3.	Add DAG and Spark YAML into the Airflow dags folder (auto-synced via git-sync).
	4.	Trigger DAG in Airflow UI â†’ watch Spark job run in spark-operator namespace.

â¸»
